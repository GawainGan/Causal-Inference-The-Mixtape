# 2. 概率与回归回顾 (Probability and Regression Review)

数值永远是抽象的，它们没有情感。但是如果你过分施压，即是是数值也有极限
-- Mos Def

## 2.1 基础概率理论 (Basic probability theory)

实践中，因果推断基于从非常简单到极其先进的统计模型。构建这样的模型需要一些基本的概率论知识，那么让我们从一些定义开始。随机过程是可以多次重复且每次都有不同结果的过程。样本空间是随机过程所有可能结果的集合。我们区分离散和连续随机过程如下[表1](#table-1)所示。离散过程产生整数，而连续过程则产生分数。

#### <a name="table-1"></a>表1. Examples of discrete and continuous random processes.
| Description    | Type       | Potential outcomes      |
|----------------|------------|-------------------------|
| 12-sided die   | Discrete   | 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12|
| Coin           | Discrete   | Heads, Tails            |
| Deck of cards  | Discrete   | 2♠, 3♠, …, King♠, Ace♠  |
| Gas prices     | Continuous |      P ≥ 0              |

我们用两种方式定义独立事件。第一种是逻辑独立。例如，两个事件发生，但没有理由相信这两个事件会相互影响。当假设它们确实相互影响时，这是一种称为***事后因果谬误***（post hoc ergo propter hoc）的逻辑谬误，意为“因此，由此产生”。这种谬误认为，事件的时间顺序不足以说明第一件事导致了第二件事。

独立事件的第二个定义是统计独立性。我们将用一个有关有无替换抽样(sampling without replacement)的例子来说明后者。以一副随机洗好的扑克牌为例。在52张牌中，第一张牌是A的概率是多少？

<p> 
    $$  \Pr(\text{Ace}) =\dfrac{\text{Count Aces}}{\text{Sample Space}}=\dfrac{4}{52}=   \dfrac{1}{13}=0.077 $$
</p> 

*注： 注释：在一副完整的扑克牌中，共有4张A，包括黑桃A、红心A、梅花A和方块A。因此，从这副牌中随机抽取一张牌，得到一张A的概率是A的总数（4张）除以牌的总数（52张）。这个计算反映了在没有任何先验信息的情况下，抽到一张A的概率。*

样本空间中有52种可能的结果，或随机过程的所有可能结果的集合。在这52种可能的结果中，我们关心A出现的频率。牌堆中有4张A，所以4/52=0.077。

假设第一张牌是A。现在我们再次提出这个问题。如果我们洗牌，那么抽到的下一张牌也是A的概率是多少？由于我们没有进行有替换的抽样，所以概率不再是1/13。我们进行了***无替换的抽样***(sampled without replacement)。因此新的概率是

<p> 
    $$ \Pr\Big(\text{Ace}\mid\text{Card } 1 =\text{Ace}\Big) = \dfrac{3}{51}= 0.059 $$
</p> 

在无替换抽样的情况下，"第一张牌是A"和"如果第一张牌是A，那么第二张牌也是A"这两个事件并不独立。为了让这两个事件变得独立，你需要将A牌放回牌堆并重新洗牌。因此，只有在以下条件下，两个事件A和B才是独立的：

<p> 
    $$ \Pr(A\mid B)=\Pr(A) $$
</p> 

两个独立事件的一个例子可能是在用另一个骰子掷出3后，用一个骰子掷出5。这两个事件是独立的，所以不管我们第一次掷出什么，掷出5的概率始终是0.17。
<p> 
    $$ 1/6 = 0.16\overline{6}  $$
</p> 

但如果我们想知道某个事件发生的概率，而这需要多个事件首先发生怎么办？例如，我们说的是克利夫兰骑士队赢得NBA冠军。2016年，金州勇士队在七场四胜制的季后赛中以3-1领先。勇士队输掉季后赛需要发生什么？骑士队必须连赢三场。在这种情况下，要找到概率，我们必须将所有边际概率的乘积相乘，或 <img src="https://latex.codecogs.com/png.latex?Pr(\cdot)^n" alt="Pr(·)^n" style="vertical-align: middle;" /> ，其中 <img src="https://latex.codecogs.com/png.latex?Pr(\cdot)" alt="Pr(·)" style="vertical-align: middle;" /> 是一个事件发生的边际概率，<img src="https://latex.codecogs.com/png.latex?n" alt="n" style="vertical-align: middle;" /> 是那个事件重复的次数。如果骑士队赢得一场比赛的无条件概率是0.5，并且每场比赛都是独立的，那么骑士队从3-1的劣势中反弹赢得冠军的概率就是每场比赛获胜概率的乘积：


<p> 
    $$ \text{Win probability} =\Pr\big(W,W,W\big)= (0.5)^3= 0.125 $$
</p> 
  
*注：也就是说，骑士队从3-1的劣势反弹赢得系列赛的概率是12.5%。*

再举一个例子，有助于理解。在德州扑克中，每位玩家面朝下发两张牌。当你手里有两张相同的牌时，你说你有两张“口袋牌”。那么，被发到口袋A的概率是多少呢？它是 
<p> 
    $$  \dfrac{4}{52}\times\dfrac{3}{51}=0.0045 $$
</p> 

没错，概率是0.45%。


让我们将我们所讨论的内容形式化，以更一般化的案例来阐述。对于独立事件，要计算联合概率，我们需要将边际概率相乘：

其中 Pr(A,B) 是事件 A 和 B 同时发生的联合概率，Pr(A) 是事件 A 发生的边际概率。

现在，让我们来看一个稍微难一点的应用。使用两个六面骰子掷出7点的概率是多少，它和掷出3点的概率相同吗？为了回答这个问题，我们比较两个概率。我们将使用一个表格来帮助解释直觉。首先，让我们看看使用两个六面骰子掷出7点的所有可能方式。掷两个骰子总共有36种可能的结果（6^2=36）。在[表2.2](#table-2-2)  中，我们看到使用两个骰子掷出7点有六种不同的方式。因此，掷出7点的概率是6/36=16.67%。接下来，让我们看看使用两个六面骰子掷出3点的所有方式。[表2.3](#table-2-3)  显示，使用两个六面骰子掷出3点只有两种方式。因此，掷出3点的概率是2/36=5.56%。所以，掷出7点和掷出3点的概率是不同的。

#### <a name="table-2-2"></a>表2.2: Total number of ways to get a 7 with two six-sided dice.
| Die 1 | Die 2 | Outcome |
|-------|-------|---------|
|1|6|7|
|2|5|7|
|3|4|7|
|4|3|7|
|5|2|7|
|6|1|7|

#### <a name="table-2-3"></a>表 2.3: Total number of ways to get a 3 using two six-sided dice.
| Die 1 | Die 2 | Outcome |
|-------|-------|---------|
|1|2|3|
|2|1|3|


## 2.2 事件和条件概率 (Events and conditional probability)
事件和条件概率。在我们讨论表示概率的三种方式之前，我想先介绍一些新的术语和概念：事件(events)和条件概率(conditional probabilities)。设 A 是某个事件，B 是另一个事件。对于两个事件，有四种可能性：

- A 和 B：A 和 B 都发生。
- ∼A 和 B：A 不发生，但 B 发生。
- A 和 ∼B：A 发生，但 B 不发生。
- ∼A 和 ∼B：A 和 B 都不发生。

我将使用几个不同的例子来说明如何表示概率。


## 2.3 概率树 (Probability tree)

让我们想象一个你正在尝试获取驾驶执照的情况。假设为了获得驾驶执照，你必须通过笔试和路考。然而，如果你未通过笔试，就不能参加路考。我们可以在概率树中表示这两个事件。

<p align="center">
    <img src="https://github.com/GawainGan/Causal-Inference-The-Mixtape/blob/main/Source/probability_tree.png?raw=true" style="width: 70%;"/>
</p>

概率树直观且易于解释<sup id="a1">[1](#f1)</sup>。首先，我们看到通过笔试的概率是0.75，而未通过的概率是0.25。其次，从每个节点分叉出去时，我们可以进一步看到与给定分支相关的概率之和为1.0。联合概率之和也为1.0。这称为全概率定律，等于所有事件A和B发生的联合概率之和：
<p> 
    $$ \Pr(A)=\sum_n Pr(A\cap B_n) $$
</p> 

我们还在驾照树中看到了条件概率的概念。例如，在通过了笔试的条件下，未通过路考的概率表示为:
<p> 
    $$ \Pr(未通过 | 通过) = \dfrac{0.45}{0.75} = 0.6 $$
</p> 

## 2.3 维恩图和集合 (Venn diagrams and sets)

维恩图是第二种用于表示多个事件发生的方式。维恩图最初由约翰·文恩在1880年构想出来。它们被用来教授基础集合论，以及在概率统计中表达集合关系。这个例子将涉及两个集合，A和B。

德克萨斯大学的足球教练整个赛季都与体育总监和校董会处于紧张关系。经过几个平庸的赛季，他与学校的未来岌岌可危。如果长角牛队无法进入重要的冠军碗赛事（美国大学没事橄榄球比赛），他可能不会被重新聘用。但如果他们能进入，他很可能会被重新聘用。让我们用这位教练的情况作为引导例子来讨论基础集合论。但在我们这样做之前，让我们回顾一下我们的术语。A和B是事件(events)，U是A和B是其子集的全集(U is the universal set of which A and B are subsets)。设A为长角牛队被邀请参加重要冠军碗赛事的概率，B为他们的教练被重新聘用的概率。设Pr(A)=0.6，Pr(B)=0.8。设A和B同时发生的概率为Pr(A,B)=0.5。

注意，A + ～A = U，其中～A是A的补集(complement)。补集意味着它是全集中不是A的所有内容(everything in the universal set that is not A)。B也是如此。B和～B的和 = U。因此：

<p> 
    $$ A+{\sim} A=B+ {\sim} B $$
</p> 

我们可以重新写出以下定义：

<p> 
    $$ \begin{align}
A & = B+{\sim} B - {\sim} A \
B & = A+{\sim} A - {\sim} B
\end{align} $$
</p> 

每当我们想要描述一组事件，其中A或B可能发生时，它就是：A ∪ B。这读作“A并B(A union B)”，意味着它是一个新的集合，包含A和B中的每个元素。因此，如果某个元素在集合A或集合B中，那么它也在新的并集(union set)中。每当我们想要描述一起发生的一组事件——即交集(joint set)时，它就是A ∩ B，读作“A交B(A intersect B)”。这个新的集合包含同时在A和B集合中的每个元素。也就是说，只有同时在A和B中的元素才会被添加到新集合中。

现在让我们仔细看看涉及集合A的关系。

<p> 
    $$ \begin{align}
A=A \cap B+A \cap{\sim} B
\end{align} $$
</p> 

注意这个表达式的意思：有两种方法可以识别A集合。首先，你可以查看所有A与B一起发生的实例。但是，A中不在B中的其余部分怎么办？好吧，那是A ∪ B的情况，它覆盖了A集合的其余部分。

类似的推理方式可以帮助你理解以下表达式。

<p> 
    $$ A \cup B=A \cap{\sim}B+\sim A \cap B+A \cap B $$
</p> 

为了得到A交B，我们需要三个对象：B之外的A单元集合(set of A units outside of B)，A之外的B单元集合(set of B units outside A)，以及它们的交集(joint set)。你得到了所有这些，你就有了A ∩ B。

现在只需简单的加法就能找到所有缺失的值。回想一下，A是你的团队进入季后赛的概率，Pr(A)=0.6。B是教练被重新聘用的概率，Pr(B)=0.8。同时，Pr(A,B)=0.5，这是A和B同时发生的概率。那么我们有：

<p> 
    $$ \begin{align}
   A              & = A \cap B+A \cap{\sim}B \\
   A \cap{\sim}B  & = A - A \cap B           \\
   \Pr(A, \sim B) & = \Pr(A) - Pr(A,B)       \\
   \Pr(A,\sim B)  & = 0.6 - 0.5              \\
   \Pr(A,\sim B)  & = 0.1                    
\end{align} $$
</p> 

在处理集合时，重要的是要理解，概率是通过考虑集合（例如A）由子集（例如A ∪ B）构成的比例来计算的。当我们说明A ∪ B发生的概率时，是相对于***U*** 而言的。但如果我们要问的问题是“就是在A发生的前提下，A和B的并集占A的哪一部分？(What share of A is due to A ∪ B?)”注意，那么我们需要这样做：

<p> 
    $$ \begin{align}
   ? & = A \cap B \div A \\
   ? & = 0.5 \div 0.6    \\
   ? & = 0.83            
\end{align} $$
</p> 

我故意在左边未定义这个问题，以便专注于计算本身。但现在让我们定义我们想要计算的内容：在A发生的情况下，B也发生的概率是多少？这就是：

<p> 
    $$ \begin{align}
   \Pr(B \mid A)  & = \dfrac{\Pr(A,B)}{\Pr(A)}= \dfrac{0.5}{0.6}=0.83 \\
   \Pr(A \mid B) & = \dfrac{\Pr(A,B)}{\Pr(B)}= \dfrac{0.5}{0.8}=0.63
\end{align} $$
</p> 

注意，这些条件概率在文氏图中并不容易看出。我们实际上是在询问子集（例如Pr(A)）中由于交集（例如Pr(A,B)）造成的百分比是多少。这种推理正是用来定义条件概率概念的相同推理。

*注：在第二个公式Pr(A|B)中，计算的顺序与Pr(B∣A)中的不同，因为我们是在已知B发生的条件下计算A发生的概率，这与已知A发生的条件下计算B的情况相反，因此结果自然也不相同。这种理解是建立条件概率概念的基础。*


## 2.5 列联表 (Contingency Tables)

我们还可以使用列联表来表示事件。列联表有时也称为双向表(twoway tables)。[表2.4](#table-2-4) 是一个列联表的示例。我们继续以担心的德克萨斯教练的例子为例。

#### <a name="table-2—4"></a>表2.4: 列联表 (Twoway contingency table)
| Event labels     | Coach is rehired (\~B) | Coach is not rehired (B) | Total |
|------------------|------------------------|--------------------------|-------|
| (A) Bowl game    | Pr(A,\~B) = 0.1        | Pr(A,B) = 0.5            | Pr(A) = 0.6 |
| (\~A) No bowl game | Pr(\~A,\~B) = 0.1      | Pr(\~A,B) = 0.3          | Pr(\~A) = 0.4 |
| Total            | Pr(\~B) = 0.2          | Pr(B) = 0.8              | 1.0   |

回想一下，Pr(A)=0.6，Pr(B)=0.8，且Pr(A,B)=0.5。注意，要计算条件概率，我们必须知道所讨论元素的频率（例如，Pr(A,B)）相对于某个更大事件的频率（例如，Pr(A)）。所以，如果我们想知道给定A条件下B的条件概率，那么它是：

<p> 
    $$ \Pr(B\mid A)=\dfrac{\Pr(A,B)}{\Pr(A)}=\dfrac{0.5}{0.6}=0.83 $$
</p> 

但请注意，知道在B发生的情况下A∪B的频率是要问以下问题：

<p> 
    $$ \Pr(A\mid B)=\dfrac{\Pr(A,B)}{\Pr(B)}= \dfrac{0.5}{0.8}=0.63 $$
</p> 

因此，我们可以使用迄今为止所做的工作来写出联合概率的定义。让我们先从条件概率的定义开始。给定两个事件A和B：

<p> 
    $$ \begin{align}
   \Pr(A\mid B) &= \frac{\Pr(A,B)}{\Pr(B)} \\
   \Pr(B\mid A) &= \frac{\Pr(B,A)}{\Pr(A)} \\
   \Pr(A,B)     &= \Pr(B,A) \\
   \Pr(A)       &= \Pr(A,\sim B)+\Pr(A,B) \\
   \Pr(B)       &= \Pr(A,B)+\Pr(\sim A, B)                   
\end{align} $$
</p> 

使用公式 **(2.1)** 和 **(2.2)**，我可以简单地写下联合概率的定义。
<p> 
    $$ \Pr(A,B) = \Pr(A \mid B) \Pr(B)
\tag{2.1} $$
</p> 
<p> 
    $$ \Pr(B,A) = \Pr(B \mid A) \Pr(A)
\tag{2.2} $$
</p> 

这就是联合概率的公式。给定公式 **(2.3)**，并使用(Pr(A,B)和Pr(B,A))的定义，我还可以重新排列项，进行替换，并将其重写为：

<p> 
    $$ \Pr(A\mid B)\Pr(B) = \Pr(B\mid A)\Pr(A)$$
</p> 
<p> 
    $$ \Pr(A\mid B) = \frac{\Pr(B\mid A) \Pr(A)}{\Pr(B)}
\tag{2.3} $$
</p> 

公式 **(2.3)** 有时被称为贝叶斯定理的朴素版本。不过，我们现在将通过将公式 **(2.5)** 代入公式 **(2.3)** 中来更全面地分解这个公式。

<p> 
    $$ \Pr(A\mid B) = \frac{\Pr(B\mid A)\Pr(A)}{\Pr(A,B) + \Pr(\sim A,B)}
\tag{2.4} $$
</p> 

将公式 **(2.1)** 代入公式 **(2.4)** 的分母中，得到：

<p> 
    $$ \Pr(A\mid B)=\frac{\Pr(B\mid A)\Pr(A)}{\Pr(B\mid A)\Pr(A)+\Pr(\sim A, B)}
\tag{2.5} $$
</p> 

接着，我们注意到使用联合概率的定义，即 Pr(B,∼A) = Pr(B |∼A)Pr(∼A)，我们将其代入公式 **(2.5)** 的分母中得到：

<p> 
    $$ \Pr(A\mid B)=\frac{\Pr(B\mid A)\Pr(A)}{\Pr(B\mid A)\Pr(A)+\Pr(B\mid \sim A)\Pr(\sim A)}
\tag{2.6} $$
</p> 

这是一系列复杂的代入，那么公式 **(2.6)** 意味着什么呢？这是贝叶斯规则的贝叶斯分解版本。让我们再次使用我们的例子，即德克萨斯州成功参加了一场重要的碗赛。A是德克萨斯州成功参加了一场重要的碗赛，B是教练被重新雇用。而A∩B是这两个事件同时发生的联合概率。我们可以使用列联表来进行每个计算。这里的问题是：如果德克萨斯州的教练被重新雇用，那么长角牛队成功参加了一场重要的碗赛的概率是多少？或者正式地表达，Pr(A | B)。我们可以使用贝叶斯分解来找到这个概率。

<p> 
    $$ \begin{align}
   \Pr(A\mid B) & = \dfrac{\Pr(B\mid A)\Pr(A)}{\Pr(B\mid A)\Pr(A)+\Pr(B\mid \sim A)\Pr(\sim A)} \\
                & =\dfrac{0.83\cdot 0.6}{0.83\cdot 0.6+0.75\cdot 0.4} \\
                & =\dfrac{0.498}{0.498+0.3} \\
                & =\dfrac{0.498}{0.798} \\
   \Pr(A\mid B) & =0.624                                                                        
\end{align} $$
</p> 

使用联合概率的定义，对照列联表检验：

<p> 
    $$ \begin{align}
   \Pr(A\mid B)=\dfrac{\Pr(A,B)}{\Pr(B)}= \dfrac{0.5}{0.8}=0.625
\end{align} $$
</p> 

所以，如果教练被重新雇用，我们成功参加了一场重要的冠军碗赛事的概率是63%。

## 2.6  蒙提霍尔问题/三门问题（Monty Hall example）

让我们用Monty Hall 例子(三门问题)作为另外一个例子，。由于大多数人会觉得它是违反直觉的，便使这个问题显得较为有趣。这个问题甚至难倒了数学家和统计学家们。但是贝叶斯定律使得答案变的非常清晰——事实上，贝叶斯法则是如此清晰，以至于让人感到惊讶的是该法竟然曾一度引起过争议(Mcgrayne 2012)。

在这个问题中，让我们先假设有三扇关闭的门：门1（D1）、门2（D2）和门3（D3）。在其中一扇门后面有一百万美元。在其他两扇门后面各有一只山羊。在这个例子中，游戏节目主持人Monty Hall 让参赛者选择一扇门。在参赛者选择完这扇门，且在主持人打开被选择的门之前，主持人会打开另一扇门来展示其门后的一只山羊。然后主持人询问问参赛者：“你想要换门吗？”

对Monty Hall的提议，一个常见的反应是认为换门毫无意义，因为两扇门后面有一百万美元的机会是相等的。因此为什么要换呢？选中的门后面有50-50的机会有一百万美元，剩下的门后面也有50-50的机会，以换门这个行为并没有什么合理性。（吗？）然而，零星的直觉会告诉你这不是正确答案，因为当Monty Hall打开那第三扇门时，他实际上生成了一个声明(statement)。但他到底说了什么呢？

让我们使用概率符号来规范化这个问题。假设你选择了门1(D1)。当你做出那个选择时，D1后面有一百万美元的概率是Pr(D1 = 1 million) = 1/3。我们将这个事件称为A1。当游戏开始时，由于样本空间中(sample space)有3扇门，且其中一扇门的后面有一百万美元，因此D1后面有一百万美元的概率为1/3。我们可以对其表示为：Pr(A1) = 1/3。另外，根据全概率定律(law of total probability)，Pr(∼ A1) = 2/3。现在的情况是，Monty Hall 打开了门2(D2)，展示了一只山羊。然后他问：“你想换到门3号吗？”

我们需要了解门3（D3）背后有一百万美元的概率，并将其与门1(D1)的概率进行比较。我们将打开门2（D2）的事件称为事件B。我们将门i（Di）背后有一百万美元的概率记为 Ai 。现在，我们将正式表述刚才的问题，并使用贝叶斯分解进行解构。我们最终想知道的是一个条件概率问题：在Monty Hall打开门2（事件B）的条件下，门1（D1）背后有一百万美元的概率（事件A1）是多少。让我们利用贝叶斯分解来表达这个条件概率。

<p> 
    $$ \Pr(A_1 \mid B) = \dfrac{\Pr(B\mid A_1) \Pr(A_1)}{\Pr(B\mid A_1) \Pr(A_1)+\Pr(B\mid A_2) \Pr(A_2)+\Pr(B\mid A_3) \Pr(A_3)}
\tag{2.7} $$
</p> 

等式右侧基本上有两种概率。一是给定门后面有一百万美元的边缘概率(marginal probability) Pr(Ai)。二是Monty Hall在门Ai后面有一百万美元的情况下，打开门2的条件概率，Pr(B | Ai)。

在没有任何额外信息的情况下，门i后面有一百万美元的边缘概率是1/3。我们称这为 ***先验概率***(prior probability)或 ***先验信念***(proior belief)。它也可以称为 ***无条件概率***(unconditional probability)。

条件概率Pr(B|Ai)需要更仔细的思考。首先看第一个条件概率Pr(B | A1)。如果门1后面有一百万美元，Monty Hall打开门2的概率是多少？

再考虑第二个条件概率Pr(B | A2)。如果钱在门2后面，Monty Hall打开门2的概率是多少？

最后一个条件概率Pr(B | A3)。如果钱在门3后面的情况下，Monty Hall打开门2的概率是多少？

每个条件概率都需要仔细考虑所涉及事件的可行性。我们来看最简单的问题：Pr(B | A2)。如果钱在门2后面，Monty Hall打开同一扇门，门2的可能性有多大？记住：这是一个游戏节目。这可以给你一些关于游戏节目主持人行为方式的线索。你认为Monty Hall会打开有一百万美元的门吗？认为他会打开实际上有钱的门是没有道理的——他总是会打开有山羊的门。那么，难道你不认为他只能打开有山羊的门吗？如果我们将这种直觉推到逻辑极限(logical extreme)，并得出结论说Monty Hall ***从不*** 打开有一百万美元的门，他 ***只会*** 打开有山羊的门。在这个假设下，我们可以通过将Pr(B | Ai)和Pr(Ai)的值代入公式 **(2.7)** 的右侧来估算Pr(A1 | B)。

那么Pr(B | A1)是多少呢？也就是说，在你选择了门1，并且钱就在门1后面的情况下，主持人打开门2的概率是多少？如果钱在门1后面，主持人便有两种选择——打开门2或门3，因为这两扇门后面都有山羊。所以Pr(B | A1) = 0.5。

那第二个条件概率Pr(B | A2)呢？如果钱在门2后面，他打开门2的概率是多少？根据我们的假设，如果门后面有一百万美元，他就不会打开那扇门，所以这个概率是0.0。最后，第三个概率Pr(B | A3)呢？如果钱在门3后面，他打开门2的概率是多少？这个要仔细考虑——参赛者已经选择了门1，所以他不能打开那扇门。他也不能打开门3，因为那扇门后面有钱。因此，他唯一能打开的门就是门2。因此，这个概率是1.0。此外，所有的边际概率Pr(Ai)都等于1/3，允许我们通过替换、乘法和除法来解出左边的条件概率。

```
简化上述内容
1. 阶段1-玩家选择了门1(D1)
2. 阶段二-主持人开别的门进行展示（和后续的询问是否要换）
这里针对的是主持人在条件1的情况下打开门2的概率-> 打开门2（事件B）
    2.1 门1后面有钱，主持人打开门2/3的概率：0.5
    2.2 门2后面有钱，主持人打开门2的概率：0 （因为主持人必然打开没有钱的那扇门）
    2.3 门3后面有钱，主持人打开门2的概率：1 （因为玩家已经选择了1，则门1不能在该阶段打开）
```

<p> 
    $$ 
\begin{align}
   \Pr(A_1\mid B) & = \dfrac{\dfrac{1}{2}\cdot                       
   \dfrac{1}{3}}{\dfrac{1}{2}\cdot
   \dfrac{1}{3}+0\cdot\dfrac{1}{3}+1.0 \cdot \dfrac{1}{3}}
   \\
                  & =\dfrac{\dfrac{1}{6}}{\dfrac{1}{6}+\dfrac{2}{6}}
   \\
                  & = \dfrac{1}{3}                                   
\end{align} $$
</p> 

啊哈。这不是有点令人惊讶吗？参赛者选择正确门的概率是1/3，就像Monty Hall打开门2之前一样。

但是，门3（你现在持有的门）后面有一百万美元的概率呢？现在门2被排除在外，你对这个可能性的看法有变化吗？让我们运用贝叶斯分解看看我们是否学到了什么。

<p> 
    $$ \begin{align}
   \Pr(A_3\mid B) & = \dfrac{ \Pr(B\mid A_3)\Pr(A_3) }{ \Pr(B\mid A_3)\Pr(A_3)+\Pr(B\mid A_2)\Pr(A_2)+ \Pr(B\mid A_1)\Pr(A_1) }      
   \\
                  & = \dfrac{ 1.0 \cdot \dfrac{1}{3} }{ 1.0 \cdot \dfrac{1}{3}+0 \cdot \dfrac{1}{3}+\dfrac{1}{2} \cdot \dfrac{1}{3}}
   \\
                  & = \dfrac{2}{3}                                                                                                   
\end{align} $$
</p> 

有趣的是，虽然你对最初选择的门的看法没有改变，但你对另一扇门的看法已经改变了。先验概率Pr(A3)= 1/3，通过一个称为更新的过程增加到了新的概率Pr(A3 | B) = 2/3。这个新的条件概率被称为***后验概率***(posterior probability)或***后验信念(posterior belief)***。这仅仅意味着在目睹了事件B（**即：支持人打开了门B并展现出门B后的山羊**）之后，你学到了一些信息，使你能够形成一个关于钱可能在哪扇门后面的新信念。

正如注释<sup id="a2">[2](#f2)</sup>中提到的关于vos Sant正确推理需要更换门的争议，基于贝叶斯定律的推断经常令聪明人感到惊讶——可能是因为我们缺乏一种合理的方式来正确地将信息纳入概率中。贝叶斯定律向我们展示了如何以一种逻辑和准确的方式做到这一点。但除了富有洞察力外，贝叶斯定律还为我们打开了一扇不同的关于因果推理的大门。虽然这本书的大部分内容都是关于从已知原因估计效果，但贝叶斯定律提醒我们，我们可以从已知的效果中形成关于原因的合理信念。

## 2.7 求和运算符(Summation operator)

我们用来推理因果关系的工具基于概率的基础之上。我们常常会使用到统计学中的数学工具和概念，比如期望(expectations)和概率。在这本书中，我们最常用的工具之一是线性回归模型(linear regression model)，但在我们深入讨论之前，我们需要构建一些简单的符号体系。我们将从求和运算符开始。希腊字母<span>Σ</span>（大写的Sigma）代表求和运算符。设<span>x<sub>1</sub></span>, <span>x<sub>2</sub></span>, ..., <span>x<sub>n</sub></span>是一系列数字。我们可以使用求和运算符来紧凑地写出数字的和：
<p>
    $$ \sum_{i=1}^nx_i \equiv x_1+x_2+\ldots+x_n $$
</p>

字母i被称为求和的索引(index)。有时也会使用其他字母，如j或k，作为求和的索引。下标变量简单地代表一个随机变量x的特定值。数字1和n分别是求和的下限和上限。表达式<span>Σ</span><sub>i=1</sub><sup>n</sup><span>x<sub>i</sub></span>可以用语言描述为“对所有i的值从1到n，求<span>x<sub>i</sub></span>的和”。一个例子可以帮助澄清：

<p>
    $$ \sum_{i=6}^9 x_i= x_6+x_7+x_8+x_9 $$
</p>

求和运算符有三个特性(properties)。第一个特性被称为常数规则(constant rule)。正式地说，它是：

<p>
    $$ \text{对于任何常数 } c: \sum_{i=1}^n c = nc $$
</p>

让我们考虑一个例子。假设我们给定：

<p>
    $$ \sum_{i=1}^3 5=(5+5+5) = 3 \cdot 5=15 $$
</p>

求和运算符的第二个特性是：

<p>
    $$ \sum_{i=1}^n c x_i = c\sum_{i=1}^nx_i $$
</p>

再次让我们用一个例子。假设我们给定：

<p>
    $$
    \begin{align}
       \sum_{i=1}^3 5x_i & =5x_1+5x_2+5x_3   \\
        & =5 (x_1+x_2+x_3)  \\
        & =5\sum_{i=1}^3x_i
    \end{align}
    $$
</p>

我们可以应用这两个特性得到以下第三个特性：
<p>
    $$
    \begin{align}
       \text{对于任何常数 } a \text{ 和 } b: \quad \sum_{i=1}^n(ax_i+by_i) =a\sum_{i=1}^n x_i + b\sum_{j=1}^n y_i
    \end{align}
    $$
</p>

在结束求和运算符之前，注意一些不是这个运算符特性的事情也是有用的。首先，比率的求和不是求和本身的比率(**the summation of a ratio is not the ratio of the summations themselve**)。

<p>
    $$
    \begin{align}
       \sum_i^n \dfrac{x_i}{y_i} \ne \dfrac{ \sum_{i=1}^n x_i}{\sum_{i=1}^ny_i}
    \end{align}
    $$
</p>

其次，某个变量平方的求和不等于其求和的平方(**the summation of some squared variable is not equal to the squaring of its summation**)。

<p>
    $$
    \begin{align}
       \sum_{i=1}^nx_i^2 \ne
       \bigg(\sum_{i=1}^nx_i \bigg)^2
    \end{align}
    $$
</p>

我们可以使用求和指示符进行许多计算，其中一些我们将在这本书的过程中反复进行。例如，我们可以使用求和运算符来计算平均值：

<p>
    $$
    \begin{align}
       \overline{x} & = \dfrac{1}{n} \sum_{i=1}^n x_i \\
    & =\dfrac{x_1+x_2+\dots+x_n}{n}   
    \end{align}
    $$
</p>

其中<span>x</span>是随机变量<span>x<sub>i</sub></span>的平均值（均值）。我们可以进行的另一个计算是一个随机变量与其自身均值的偏差。偏差之和总是等于0：

<p>
    $$ \sum_{i=1}^n (x_i - \overline{x})=0 $$
</p>

你可以通过[表2.5](#table-2-5)来直观的感受。

#### <a name="table-2—5"></a>表2.5：偏差之和等于零 (Sum of deviations equalling zero)
|  x  | x-<span style="text-decoration: overline">x</span> |
|-----|----------------|
| 10  |       2        |
|  4  |      -4        |
| 13  |       5        |
|  5  |      -3        |
| 平均值=8 |     和=0     |


考虑两个串数字<span>y<sub>1</sub></span>, <span>y<sub>2</sub></span>, ..., <span>y<sub>n</sub></span>和<span>x<sub>1</sub></span>, <span>x<sub>2</sub></span>, ..., <span>x<sub>n</sub></span>。现在我们可以考虑x和y可能值的双重求和。例如，如果我们考虑在n = m = 2的情况下。<span>Σ</span><sub>i=1</sub><sup>2</sup><span>Σ</span><sub>j=1</sub><sup>2</sup><span>x<sub>i</sub>y<sub>j</sub></span> 等于<span>x<sub>1</sub>y<sub>1</sub></span>+<span>x<sub>1</sub>y<sub>2</sub></span>+<span>x<sub>2</sub>y<sub>1</sub></span>+<span>x<sub>2</sub>y<sub>2</sub></span>。这是因为
<p>
    $$
    \begin{align}
       x_1y_1+x_1y_2+x_2y_1+x_2y_2
         & = x_1(y_1+y_2)+x_2(y_1+y_2)                     \\
         & = \sum_{i=1}^2x_i(y_1+y_2)                      \\
         & = \sum_{i=1}^2x_i \bigg( \sum_{j=1}^2y_j \bigg) \\
         & = \sum_{i=1}^2 \bigg( \sum_{j=1}^2x_iy_j \bigg) \\
         & = \sum_{i=1}^2 \sum_{j=1}^2x_iy_j               
    \end{align}
    $$
</p>

整本书中非常有用的一个结果是：

<p>
    $$ \sum_{i=1}^n(x_i - \overline{x})^2=\sum_{i=1}^n x_i^2 - n(\overline{x})^2 \tag{2.8} $$
</p>

一个过于冗长的、逐步证明如下。注意求和索引在第一行之后被省略，以便更容易阅读。

<p>
    $$
    \begin{align}
      \sum_{i=1}^n(x_i-\overline{x})^2 & =                                                                                               
      \sum_{i=1}^n (x_i^2-2x_i\overline{x}+\overline{x}^2) \\
      & = \sum x_i^2 - 2\overline{x} \sum x_i +n\overline{x}^2                                          \\
      & = \sum x_i^2 - 2 \dfrac{1}{n} \sum x_i \sum x_i +n\overline{x}^2                                \\
      & = \sum x_i^2 +n\overline{x}^2 - \dfrac{2}{n} \bigg (\sum x_i \bigg )^2                          \\
      & = \sum x_i^2 - n \bigg (\dfrac{1}{n} \sum x_i \bigg )^2                                         \\
      & = \sum x_i^2 - n \overline{x}^2                                                                 
    \end{align}
    $$
</p>

这个结果的一个更通用版本是：

<p>
    $$
    \begin{align}
       \sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y}) & = \sum_{i=1}^n x_i(y_i - \overline{y}) \\
        & = \sum_{i=1}^n (x_i - \overline{x})y_i \\
        & = \sum_{i=1}^n x_iy_i - n(\overline{xy}) \\
    \end{align}
    $$
</p>

又或者是另一种版本：

<p>
    $$
    \begin{align}
      \sum_{i=1}^n (x_i -\overline{x})(y_i - \overline{y}) &= \sum_{i=1}^n x_i(y_i - \overline{y}) \\
      &= \sum_{i=1}^n (x_i - \overline{x})y_i = \sum_{i=1}^n x_i y_i - n(\overline{x}\overline{y})
    \end{align}
    $$
</p>

## 2.8 期望值 (Expected value)
随机变量的期望值，也称为期望(expectation)或有时称为总体均值(population mean)，其实就是变量可能取到的各个值的加权平均数(weighted vaerage of the possible values)，这里的权重是由每个值在总体中出现的概率决定的。假设变量<span>X</span>可以取值<span>x<sub>1</sub></span>，<span>x<sub>2</sub></span>，...，<span>x<sub>k</sub></span>，每个值分别以概率<span>f(x<sub>1</sub>)</span>，<span>f(x<sub>2</sub>)</span>，...，<span>f(x<sub>k</sub>)</span>出现。那么我们定义<span>X</span>的期望值为：

<p>
    $$
    \begin{align}
  E(X) & = x_1f(x_1)+x_2f(x_2)+\dots+x_kf(x_k) \\
  & = \sum_{j=1}^k x_jf(x_j)              
\end{align}
    $$
</p>

让我们来看一个数值例子。如果<span>X</span>取值为-1，0和2，概率分别为0.3，0.3和0.4。那么<span>X</span>的期望值等于：

<p>
    $$
    \begin{align}
  E(X) & = (-1)(0.3)+(0)(0.3)+(2)(0.4) \\
  & = 0.5                         
\end{align}
    $$
</p>

实际上，你也可以对该变量的函数取期望，例如<span>X<sup>2</sup></span>。注意<span>X<sup>2</sup></span>只取值1，0和4，概率分别为0.3，0.3和0.4。因此，计算<span>X<sup>2</sup></span>的期望值是：

<p>
    $$
    \begin{align}
  E(X^2) & = (-1)^2(0.3)+(0)^2(0.3)+(2)^2(0.4) \\
  & = 1.9                               
\end{align}
    $$
</p>

期望值的第一个特性是，对于任何常数<span>c</span>，<span>E(c) = c</span>。第二个特性是，对于任何两个常数<span>a</span>和<span>b</span>，那么<span>E(aX+b)=E(aX)+E(b)=aE(X)+b</span>。第三个特性是，如果我们有多个常数，<span>a<sub>1</sub></span>，...，<span>a<sub>n</sub></span>和许多随机变量，<span>X<sub>1</sub></span>，...，<span>X<sub>n</sub></span>，那么以下公式成立：

<p>
    $$
    \begin{align}
   E(a_1X_1+\dots+a_nX_n)=a_1E(X_1)+\dots+a_nE(X_n)
\end{align}
    $$
</p>

我们也可以使用期望算子来表达这一点：

<p>
    $$
    \begin{align}
   E\bigg(\sum_{i=1}^na_iX_i\bigg)=\sum_{i=1}a_iE(X_i)
\end{align}
    $$
</p>

在特殊情况下，当<span>a<sub>i</sub></span> = 1时，那么

<p>
    $$
    \begin{align}
   E\bigg(\sum_{i=1}^nX_i\bigg)=\sum_{i=1}^nE(X_i)
\end{align}
    $$
</p>


## 2.9 方差 (Variance)

方差

期望运算符E(·)是一个针对总体的概念。它指的是整个感兴趣的群体(whole group of interest)，而不仅仅是我们可用的样本。它的含义与随机变量在总体中的平均值有些类似。假设有两个随机变量W和H，可以解释期望运算符的一些附加属性。

<p>
    $$
    \begin{align}
   E(aW+b)             & = aE(W)+b\ \text{for any constants $a$, $b$}
   \\
   E(W+H)              & = E(W)+E(H)                                  \\
   E\Big(W - E(W)\Big) & = 0                                          
\end{align}
    $$
</p>

考虑一个随机变量的方差，W：

<p>
    $$
    \begin{align}
   V(W)=\sigma^2=E\Big[\big(W-E(W)\big)^2\Big]\ \text{in the population}
\end{align}
    $$
</p>

我们可以用下面的公式来表示

<p>
    $$
    V(W)=E(W^2) - E(W)^2
\tag{2.9}
    $$
</p>

在给定的数据样本中，我们可以通过以下计算来估计方差：

<p>
    $$
    \begin{align}
   \widehat{S}^2=(n-1)^{-1}\sum_{i=1}^n(x_i - \overline{x})^2
\end{align}
    $$
</p>

其中我们用n − 1来除是因为我们在估计均值时做了一个自由度的调整(degree-of-freedom adjustment from estimating the mean)。但是在大样本中，这种自由度调整对<span>S<sup>2</suo></span>的值没有实际影响(no practical effect)，其中<span>S<sup>2</suo></span>是对所有平方偏差之和（在自由度校正后）的平均值

方差的几个其他特性。首先的一条方差公式是：

<p>
    $$
    V(aX+b)=a^2V(X)
\tag{2.10}
    $$
</p>

而一个常数的方差是0（即，对任何常数c，V(c) = 0）。两个随机变量之和的方差等于：

<p>
    $$
    V(X+Y)=V(X)+V(Y)+2\Big(E(XY) - E(X)E(Y)\Big)
\tag{2.11}
    $$
</p>

如果两个变量是独立的，那么E(XY)=E(X)E(Y)且V(X+ Y)等于V(X)+V(Y)之和。

## 2.10 协方差 (Covariance)

**公式2.11**的最后一部分被称为协方差。协方差衡量了两个随机变量之间的线性依赖程度。我们用<span>C(X,Y)</span>操作符来表示它。表达式<span>C(X,Y) > 0</span>表示两个变量同向移动，而<span>C(X,Y) < 0</span>表示它们反向移动。因此我们可以将**公式2.11**改写为：

<p> $$ \begin{align}
   V(X+Y)=V(X)+V(Y)+2C(X,Y)
\end{align}
    $$ </p>

虽然说零协方差意味着两个随机变量无关，这个说法很诱人，但这其实是不正确的。它们可能有非线性关系。协方差的定义是

<p> $$ C(X,Y) = E(XY) - E(X)E(Y)
\tag{2.12} $$ </p>

正如我们所说，如果X和Y是独立的，那么在总体中<span>C(X,Y) = 0</span>。两个线性函数之间的协方差是：

<p> $$ \begin{align}
   C(a_1+b_1X, a_2+b_2Y)=b_1b_2C(X,Y)
\end{align} $$ </p>

两个常数，<span>a<sub>1</sub></span>和<span>a<sub>2</sub></span>，因为它们的平均值是它们自己，所以差值等于0。

解释协方差的大小可能有些棘手。为此，我们更好地通过查看相关性来理解。我们定义相关性如下。设<span>W = \frac{X-E(X)}{\sqrt{V(X)}}</span>和<span>Z = \frac{Y-E(Y)}{\sqrt{V(Y)}}</span>。然后

<p> $$ \text{Corr}(X,Y) = \text{Cov}(W,Z)=\dfrac{C(X,Y)}{\sqrt{V(X)V(Y)}}
\tag{2.13} $$ </p>

相关系数的范围是-1到1。正相关（负相关）表明变量以相同（相反）的方式变化。系数越接近1或-1，线性关系越强。

## 2.11 人口模型 (Population model)
我们从横截面分析开始。假设我们可以从感兴趣的人群中收集一个随机样本。假设有两个变量，<span>x</span>和<span>y</span>，我们想要观察<span>y</span>随<span>x</span>的变化而变化的情况。

立即出现三个问题。一，如果<span>y</span>受到除<span>x</span>以外的其他因素的影响，我们将如何处理？二，连接这两个变量的函数形式是什么？三，如果我们对<span>x</span>对<span>y</span>的因果效应(causal effect)感兴趣，那么我们如何将其与仅仅的相关性区分开来？让我们从一个具体的模型开始。

<p> $$ y=\beta_0+\beta_1x+u
\tag{2.14} $$</p>

这个模型被假定在目标总体(population)中进行。**公式2.14**定义了一个线性的双变量回归模型。对于关注捕获因果效应的模型，左项通常被认为是"效应"或因变量（被解释变量, effect），而右项被认为是"原因"或自变量（解释变量, causes）

**公式2.14**通过包含一个称为误差项的随机变量<span>u</span>，明确允许其他因素影响<span>y</span>。这个方程也通过假设<span>y</span>与<span>x</span>线性相关，明确地模拟了函数形式。我们称<span>β<sub>0</sub></span>系数为截距参数(intercept parameter)，称<span>β<sub>1</sub></span>系数为斜率参数(slope parameter)。这些描述了一个目标总体(population)，而我们在实证工作中的目标是估计它们的值。我们永远不会直接观察到这些参数，因为它们不是数据（我将在整本书中强调这一点）。不过，我们可以使用数据和假设来估计这些参数。为此，我们需要可信的假设，以便用数据准确估计这些参数。我们稍后会回到这一点。在这个简单的回归框架中，所有未观察到的决定<span>y</span>的变量都被误差项<span>u</span>所包含。

首先，我们在不失一般性的情况下做一个简化假设。让<span>u</span>在人群中的期望值为零。形式上：

<p> $$ E(u)=0
\tag{2.15} $$</p>

其中<span>E(·)</span>是之前讨论过的期望值运算符。如果我们将<span>u</span>随机变量归一化为0，这并无大碍。为什么？因为<span>β<sub>0</sub></span>（截距项）的存在总是允许我们这样做的灵活性。如果<span>u</span>的平均值不为0——例如，假设它是<span>α<sub>0</sub></span>——那么我们调整截距。调整截距对<span>β<sub>1</sub></span>斜率参数没有影响。例如：

<p> $$ \begin{align}
   y=(\beta_0+\alpha_0)+\beta_1x+(u-\alpha_0)
\end{align} $$</p>

其中<span>α<sub>0</sub></span> = <span>E(u)</span>。新的误差项是<span>u−α<sub>0</sub></span>，新的截距项是<span>β<sub>0</sub> + α<sub>0</sub></span>。但是虽然这两个术语发生了变化，注意没有变化的是：斜率，<span>β<sub>1</sub></span>。

## 2.12 均值独立 (Mean Independence)


<sup id="a0">[0](#f0)</sup>
***
## 笔记

<p> $$  $$</p>

[表0.0](#table-0-0)
#### <a name="table-0—0"></a>表0.0: xxx表 (xxx table)

<b>注释 0:</b>
<b id="f0">""</b> [↩](#a0)
<p> </p>


<b>注释 1:</b>
<b id="f1">The set notation <img src="https://latex.codecogs.com/png.latex?%5Ccup" alt="\cup" /> means “union” and refers to two events occurring together. 
集合符号 <img src="https://latex.codecogs.com/png.latex?%5Ccup" alt="\cup" /> 的意思是 "联合"，指两个事件同时发生。</b> [↩](#a1)
<p> </p>

<b>注释 2:</b>
<b id="f2">"有这样一个具有讽刺意味的故事：有人向专栏作家玛丽莲-沃斯-萨凡特（Marilyn vos Savant）提出了蒙蒂-霍尔（Monty Hall）的问题。沃斯-萨凡特智商极高，因此人们会送难题来难倒她。在没有贝叶斯分解法的情况下，她只用逻辑思维就答对了。不过，她的专栏激怒了人们。评论家们纷纷来信抱怨她错得有多离谱，但事实上错的是他们自己"</b> [↩](#a2)
<p> </p>


